---
title: 'Homework ASM (SoSe 2021) -- Week 7'
author: "Marc Blauert"
date: "2021-05-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(dirname(rstudioapi::getSourceEditorContext()$path))

rm(list = ls())
setwd(dirname(rstudioapi::getSourceEditorContext()$path))

library(tidyverse)
library(brms)
library(rethinking)
library(bayesplot)
library(tidybayes)
library(rcartocolor)
```

```{r, include=FALSE}
theme_plots <- function(){
  theme_classic()+
    theme(axis.text.x = element_text(size = 12, angle = 45, vjust = 1, hjust = 1),
          axis.text.y = element_text(size = 12),
          axis.title = element_text(size = 12),
          panel.grid = element_blank(),
          plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , "cm"),
          plot.title = element_text(size = 14, vjust = 1, hjust = 0.5),
          legend.text = element_text(size = 12, face = "italic"),
          legend.title = element_blank(),
          legend.position = "right")
}
```

### Tasks 1: McElreath Ch. 7.7, Ex. 7H3

Consider three fictional Polynesian islands. On each there is a Royal Ornithologist charged by the king with surveying the bird population. They have each found the following proportions of 5 important bird species (see the table constructed in the following). Notice that each row sums to 1, all the birds. This problem has two parts. It is not computationally complicated. But it is conceptually tricky. 

* First, compute the entropy of each island’s bird distribution. Interpret these entropy values. 
* Second, use each island’s bird distribution to predict the other two. This means to compute the KL divergence of each island from the others, treating each island as if it were a statistical model of the other islands. You should end up with 6 different KL divergence values. Which island predicts the others best? Why?

Frst, I create the data:

```{r}
d1 <- 
  tibble(island = c("island1", "island2", "island3"), 
         species_a = c(0.2, 0.8, 0.05), 
         species_b = c(0.2, 0.1, 0.15),
         species_c = c(0.2, 0.05, 0.7),
         species_d = c(0.2, 0.025, 0.05), 
         species_e = c(0.2, 0.025, 0.05)) %>%
  column_to_rownames(var="island")

d1 %>% mutate(sum = rowSums(across(where(is.numeric)))) # check if data was copied correctly
```

Now, I can define a function for the entropy and apply it to each island (the first sub-task):

```{r}
entropy_fun <- function(p) {-sum(p*log(p))}

entropy_islands <- apply(d1, 1, entropy_fun); entropy_islands
```

Since on island1 all birds occur in the same proportion, the entropy is the highest. Contrastingly, the diverging proportions of bird species on the other two islands leads to lower entropy.

Moving on to the second sub-task:
```{r}
# (NOTE: Here, I didn't know how to continue... After spending some time on it, I decided to omit this section and will compare with the model solutions.)
```


### Tasks 2: Model comparison with the divorce rate models from Week6

Recall the divorce rate exercise from last week (Week6). Re-fit some of these models using brms, and compare them using the WAIC and PSIS-LOO estimates of ELPD. In particular, compare model m5.1 from the book with some of your models from last week including southernness as a predictor. Explain the model comparison results.

First, I load the divorce data which we also used last week:

```{r}
# Loading the data:
data(WaffleDivorce, package = "rethinking")
d2 <- WaffleDivorce
rm(WaffleDivorce)

# Standardizing the variables for divorce rate (D), marriage rate (M), and median age at marriage (A):
d2 <-
  d2 %>% 
  mutate(D = (Divorce - mean(Divorce)) / sd(Divorce)) %>% 
  mutate(M = (Marriage - mean(Marriage)) / sd(Marriage)) %>% 
  mutate(A = (MedianAgeMarriage - mean(MedianAgeMarriage)) / sd(MedianAgeMarriage))

# Creating a index variable for state type (S):
d2 <-
  d2 %>% 
  mutate(S = ifelse(South == 1, 2, 1)) %>% 
  mutate(S = factor(S))

levels(d2$S) <- c("Non-Southern state", "Southern state")
```

Now, I re-estimate the models predicting the divorce rate. I use the following four models for comparison:

(1) `fit_biv_A`: This corresponds to model m5.1 in Statistical Rethinking; `D ~ 1 + A`  
(2) `fit_biv_S`: From last weeks exercise; `D ~ 1 + S`  
(3) `fit_mlr_all`: From last weeks exercise; `D ~ 1 + M + A + S`  
(4) `fit_mlr_A_S`: Omit M from the previous model (3) since it appears to have little predictive power ; `D ~ 1 + A + S`  

```{r, results="hide"}
fit_biv_A <- 
  brm(data = d2, 
      family = gaussian,
      D ~ 1 + A,
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 0.5), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 5,
      sample_prior = T,
      file = "fits/fit_biv_A")

fit_biv_S <- 
  brm(data = d2, 
      family = gaussian,
      D ~ 1 + S,
      prior = c(prior(normal(0, 0.5), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 5,
      file = "fits/fit_biv_S")

fit_mlr_all <- 
  brm(data = d2, 
      family = gaussian,
      D ~ 1 + M + A + S,
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 0.5), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 5,
      file = "fits/fit_mlr_all")

fit_mlr_A_S <- 
  brm(data = d2, 
      family = gaussian,
      D ~ 1 + A + S,
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 0.5), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 5,
      file = "fits/fit_mlr_A_S")
```

With the models re-estimated, we can now move to the model comparison:

```{r, results="hide"}
# Store the WAIC criterion in the fit objects
fit_biv_A <- add_criterion(fit_biv_A, c(criterion = "waic", criterion = "loo"))
fit_biv_S <- add_criterion(fit_biv_S, c(criterion = "waic", criterion = "loo"))
fit_mlr_all <- add_criterion(fit_mlr_all, c(criterion = "waic", criterion = "loo"))
fit_mlr_A_S <- add_criterion(fit_mlr_A_S, c(criterion = "waic", criterion = "loo"))
```

```{r}
# Store the WAIC criterion in the fit objects
waic_comp <- loo_compare(fit_biv_A, fit_biv_S, fit_mlr_all, fit_mlr_A_S, criterion = "waic")
loo_comp <- loo_compare(fit_biv_A, fit_biv_S, fit_mlr_all, fit_mlr_A_S, criterion = "loo")

print(waic_comp, simplify = FALSE)
print(loo_comp, simplify = FALSE)
``` 

For both `WAIC` and `LOO` the results are almost identical. The `elpd_diff` expresses the difference between the four models. First, it is noticeable that all models except `fit_biv_S` do not show much of a difference in fit. However, the multivariate model omitting marriage rate (`fit_biv_S`) is found to have the best model fit.

From the loo-summary and the graph below one sees in more detail the model fit of the best fitting model `fit_mlr_A_S`:

```{r}
loo(fit_mlr_A_S)

tibble(pareto_k = fit_mlr_A_S$criteria$loo$diagnostics$pareto_k,
       p_waic   = fit_mlr_A_S$criteria$waic$pointwise[, "p_waic"],
       Loc      = pull(d2, Loc)) %>% 
  
  ggplot(aes(x = pareto_k, y = p_waic, color = Loc == "ID")) +
  geom_vline(xintercept = .5, linetype = 2, color = "black", alpha = 1/2) +
  geom_point(aes(shape = Loc == "ID")) +
  geom_text(data = . %>% filter(p_waic > 0.5),
            aes(x = pareto_k - 0.03, label = Loc),
            hjust = 1) +
  scale_shape_manual(values = c(1, 19)) +
  scale_color_brewer(palette = "Dark2") +
  labs(subtitle = "Gaussian model (fit_mlr_A_S)") +
  theme_plots() +
  theme(legend.position = "none")

``` 

Additionally, the brms-bookdown provides simple instructions to obtain $R^2$ from the fit object. Since the models are all simple single-level and Gaussian based, use of $R^2$ to investigate model fit is possible without risking to overestimate the fit of the model.

```{r}
rbind(bayes_R2(fit_biv_A), 
      bayes_R2(fit_biv_S), 
      bayes_R2(fit_mlr_all),
      bayes_R2(fit_mlr_A_S)) %>%
  data.frame() %>%
  mutate(model = c("fit_biv_A", "fit_biv_S", "fit_mlr_all", "fit_mlr_A_S"),
         r_sq_post_mean = round(Estimate, digits = 3)) %>%
  select(model, r_sq_post_mean)
``` 

Again, we can observe that all models except `fi_biv_S` have a relatively higher and comparable fit. However, the nature of $R^2$ can also be seen from the results. As a measure of how much of the variance can be explained by the predictors of the model, the `fit_mlr_all` model, in which all three predictor variables are present, is estimated to be the slightly superior model. This is despite the fact that we have seen in our exploratory tests that marriage rate has no clear effect on divorce rate when the other variables are also added to the model, leading us to omit it.
